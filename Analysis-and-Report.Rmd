---
title: "Analysis and Report"
author: "R.Riddell"
date: "2023-10-09"
output:
  html_document:
    toc: yes
    toc_depth: 1
    toc_float:
      collapsed: no
      smooth_scroll: no
    theme: flatly
    highlight: tango
  word_document: default
  pdf_document:
    toc: yes
    toc_depth: '1'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, warning=TRUE, include=FALSE}
library(tidyverse)
library(broom)
library(caret)
library(modelr)
library(randomForest)
library(scales)
library(gridExtra)                                        
```

```{r Read Data, warning=TRUE, include=FALSE}
# Reading the aggregated data. This file is generated by the Data-Cleaning.Rmd script and located in the out folder
dat <- read_csv('out/aggregated_data.csv')
```

# Introduction
The housing market changes can have ripple effects through large sections of the economy. The effects can be seen in areas like consumer spending or directly impact the construction industry. Having a broader understanding of the factors that affect housing prices can better equip people to make decisions when entering or exiting the market [RBA-2019](https://www.rba.gov.au/speeches/2019/sp-gov-2019-03-06.html#:~:text=They%20influence%20consumer%20spending%2C%20including,value%20of%20collateral%20for%20loans). This report will examine house prices concerning broader factors surrounding the housing market between 2016 - 2018 and seek to shed some light on the key drivers at that time. 

# Data
The data in this report is from publicly available sources, including the ABS (Australia Bureau of Statistics) and kaggle. The data can be found at [ABS DATA](https://www.abs.gov.au/methodologies/data-region-methodology/2011-22#data-downloads), 
[Melbourne Housing](https://www.kaggle.com/datasets/anthonypino/melbourne-housing-market), [Postcode/ LGA data](https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.006July%202011?OpenDocument). The data was aggregated to create a single data set that is used for this exploration and analysis.

The 2018 Melbourne Housing data was used as the central data source. The data was then enriched with local area data that was obtained from the ABS. The two data sets couldn't be directly connected as the area identifier on the ABS data was the LGA (Local Government Area), which wasn't in the housing data. It could have been possible to map the Label from the ABS data to the Council Area in the housing data; this would have required some text manipulation. Instead, a linking table was used. This table came from an ABS source and gives the LGA code for each postcode. With this information, the ABS data can be mapped through to the postcode field in the Melbourne housing data. The risks with this method were the postcode/LGA mapping table was produced in 2011, and I have not verified how the postcode and LGA actually line up.

The additional data from the ABS was centred around the Economy/Industry and Education/Employment based on the LGA. This information was collected at the same time as the Melbourne Housing data. When multiple years of was data avaiable, the data was averaged over 2016 - 2018. There could be additional work to look at this data in direct relation to the sale data and if lead/ lag from those values could be a predictor or a response. The LGA grouped all values, and the additional variables added were:

##### ECONOMY AND INDUSTRY, Local Government Area,  2011, 2016-2022
**Total house transfers** - Taken directly from the report\
**Total Motor Vehicles** - Taken directly from the report\
**Age of Motor Vehicles** - Calculated from columns under Registered moto vehicles - Year of manufacture - at 31 January (CS - CU). The calculation was done by getting the total of the three columns and then calcualting the percentage of vehicles that were less than five years old.\

##### EDUCATION AND EMPLOYMENT, Local Government Area,  2011, 2016-2022
**Total Number of Jobs** - Taken directly from the report(Number of employee jobs - Total (AH))\
**Most popular job occupation** - Calculated from columns under Occupation of employed persons - Persons aged 15 years and over - Census (CD - CL). The extraction of this was done by taking the label that correspondes to the highest percentage selected in each LGA.\
**Most popular job category** - Calculated from columns under Jobs in Australia - year ended 30 June (O - AG). The extraction of this was done by taking the label that correspondes to the highest number in each LGA.\


### NA Treatment

```{r NA treatment, warning=TRUE, include=FALSE}
# Setting a seed to make the random sample consistent
set.seed(42)

# Subsetting the data to speed up the analysis
dat <- dat[sample(nrow(dat), size=30000),]

# Viewing the percentage of NA values in all of the columns
na_columns <- data.frame((colSums(is.na(dat)) / nrow(dat)) * 100)
colnames(na_columns) <- 'NA_Perc'
na_columns %>% 
  arrange(-NA_Perc)

# Dropping columns based on NA values or other reasons
cols_to_drop <- c('BuildingArea', 'YearBuilt', 'Landsize', 'Car', 'Bathroom', 'Bedroom2', 'Lattitude', 'Longtitude', 'Code', 'Address', 'SellerG', 'Postcode', 'Suburb', 'CouncilArea')
dat <-  dat %>% 
  select(-1,-cols_to_drop) %>% 
  filter(!is.na(Price))

# Viewing how many distinct values apear in each column
sapply(dat, function(x) n_distinct(x))

# Dropping the observations that contain NA values
dat <- dat[complete.cases(dat),]

# Converting the Date to a date format
dat$Date <- as.Date(dat$Date, format = "%d/%m/%Y")

# Changing the Date to only be the Year
dat$Date <- format(dat$Date, "%Y")
```

After assessing all the columns that had more than 20% null values, the decision was made to drop them. It didn't make any clear sense to impute 0 into these columns; the values were then assessed against the whole data set and the other data points in their suburb. The range was reasonably significant on both counts, so mean imputation didn't seem practical. The observations that included no final price were also dropped. As the price is the aim of the investigation, it was decided not to create target values synthetically. Seven additional values were dropped as they had some NA values in the Postcode/ Property Count. This has left the data set with `r length(dat$Price)` observations and `r length(colnames(dat))` variables.

The columns dropped were `r cols_to_drop`.


# Exploratory Analysis
### Distribution
The numeric values were analysed using histograms. All were relatively normal. The average number of house transfers shows a normal distribution, with some areas having significantly more (figure 1). The ratio of vehicles under five years shows some right-hand skewness, implying that some areas have a notable proportion of vehicles under five years old (figure 2). There is no apparent reason to drop any of these values based on their distribution.
```{r histograms num vars, warning=TRUE, include=FALSE}
# Reading in the custom function to make histograms
source("funs/gg_hist.R")

# Generating the names of the columns that are numeric values
numeric_vars <- dat %>% 
  select(where(is.numeric),- c(Price)) %>% 
  names(.)
```


```{r histograms, eval=FALSE, warning=TRUE, include=FALSE}
# Creating histograms for the numeric values to assess distribution
for (i in seq_along(numeric_vars)) {
  print(gg_hist(dat, numeric_vars[i]))
}
```

```{r histograms2, echo=FALSE, warning=FALSE}
# Creating specific plots for the final reprt
ave_house_transfers_hist <- gg_hist(dat, 'ave_house_transfers')+labs(title = 'Average House \n Transfers', 
                                                                     subtitle = '',
                                                                     x = 'Number of house \n transfers by LGA',
                                                                     caption = 'figure 1')
ratio_less_five_years_car_age_hist <- gg_hist(dat, 'ratio_less_five_years_car_age') + 
                                              labs(title = 'Ratio of vehicles \n less than 5 years old', 
                                                   subtitle = '',
                                                   x = 'Percentage (%) \n',
                                                   caption = 'figure 2')  +
                                              scale_x_continuous(labels = scales::percent)

# Displaying the plots side by side
grid.arrange(ave_house_transfers_hist, ratio_less_five_years_car_age_hist, ncol = 2)
```
```{r Save histograms, include=FALSE}
# Saving the plots to the figs/ folder
ggsave(plot = ave_house_transfers_hist, filename = "figs/ave_house_transfers_hist.jpeg")
ggsave(plot = ratio_less_five_years_car_age_hist, filename = "figs/ratio_less_five_years_car_age_hist.jpeg")
```
 

The distribution of some of the categorical variables was assessed against the price. In some cases, the distribution was significant across each category and showed some potential outliers. In particular, the most common occupation shows that "Professionals" have significantly more buying potential in some cases, but their median is somewhat similar to the other professions (figure 3). When looking closer though, the data set is largely skewed by Professionals as they account for 86% of the responses. The Region Name also appears to be a factor in the price, with the regions with the higher medians having the most significant distribution. This may point to the fact that a house in every region is valued at the same price, but it could be assumed that it is of a different quality, size or age (figure 4). When analysing the distribution of values in the data set, three regions provide roughly 80% of the data: Southern Metropolitan, Northern Metropolitan and Eastern Metropolitan.
```{r box_plots, warning=TRUE, include=FALSE}
# Reading in a custom function for boxplots
source("funs/gg_box.R")

# Generating the required categorical names to make boxplots
categorical_var_names <- c("Type", "Method", "Regionname", "Label", "category", "occupation")
```

```{r box_plots print, eval=FALSE, warning=TRUE, include=FALSE}
# Creating a boxplot for all some categorical values to assess how that category discrimnates price
for (i in seq_along(categorical_var_names)) {
  print(gg_box(dat, categorical_var_names[i]))
}

# Looking at the proporting of data that reports from each Region
dat %>% 
  mutate(count = n()) %>% 
  group_by(Regionname) %>% 
  summarise(n = n()/count) %>% 
  unique() %>% 
  arrange(-n)
```

```{r box_plots2, echo=FALSE, warning=FALSE, fig.align='center'}
# Creating plots for the final report
occupation_box <- gg_box(dat, 'occupation')+labs(title = 'Common Occupations \n Price Distribution', 
                                                 subtitle = '',
                                                 y = 'Price ($)',
                                                 caption = "figure 3") + 
                                                 scale_y_continuous(labels = scales::comma) +
                                                 scale_x_discrete(labels = scales::wrap_format(20))
Regionname_box <- gg_box(dat, 'Regionname')+labs(title = 'Region Name \n Price Distribution', 
                                                 subtitle = '', 
                                                 y = 'Price ($)',
                                                 caption = 'figure 4') + 
                                              scale_y_continuous(labels = scales::comma) +
                                              scale_x_discrete(labels = scales::wrap_format(20))

# Arranging plots for the final report
grid.arrange(occupation_box, Regionname_box, ncol=2)
```

```{r save boxplots, include=FALSE}
# Saving plots to the figs/ folder
ggsave(plot = occupation_box, filename = "figs/occupation_box.jpeg")
ggsave(plot = Regionname_box, filename = "figs/Regionname_box.jpeg")
```


### Relationships
The correlation coefficient of the numeric values against the price was calculated. The Rooms variable showed the highest positive correlation with a result of 0.46, showing that a property with more rooms may result in a higher sale price. The strongest negative correlation was the Distance from the CBD; this shows a slight relationship that the sale price will be less when a house is further from the CBD. Purely from the correlation coefficients, the Property count and average total jobs have been excluded as they are between -.10 and .10.
```{r correlation, warning=TRUE, include=FALSE}
# Creating an empty space to add the correlations to
correlations <- numeric()

# Setting the dependant variable to Price
target_var <- "Price"

# Iterating through the numeric variables and assessing the correlation against Price
for (col in numeric_vars) {
  correlation <- cor(dat[[target_var]], dat[[col]])
  correlations <- c(correlations, correlation)
}

# Formatting the correlations as a data.frame
correlation_result <- data.frame(
  Variable = numeric_vars,
  Correlation = correlations
)

# sorting by correlation to assess each column's value
correlation_result %>% 
  arrange(-Correlation)

# Dropping columns that have a correlation between -.10 and .10
cols_to_drop <- c('Propertycount', 'ave_total_jobs')
dat <-  dat %>% 
  select(-1,-cols_to_drop)
```


# Assumption
Before attempting modelling, it is essential to check the data to ensure it fits the models' assumptions. This process helps to select the models that are most appropriate to the data, it will provide a more valid result and the model should be more reflective of the data and therefore more accurate. As a starting point, a linear model has been created with the price as the target variable, and it will be tested against the assumption for the linear model.
```{r linear model, eval=FALSE, warning=FALSE, include=FALSE}
# Creating a elemnt that has the Price in it and dropping Price from the larger dataframe
Price = dat$Price
dat_no_price <- dat %>% 
  select(-Price)

# Encoding the data to convert the categorical variables to numeric and preparing the dataframe for linear modelling
dummy_encoded <- model.matrix(~ . - 1, data = dat_no_price)

# Fitting a linear model
fit <- lm(Price ~ ., dat)

# Displaying the model results
tidy(fit, conf.int = T)
summary(fit)

# Saving model to the out/models folder
save(fit, file = "out/models/lm_mdl.Rdata")
```

### Continous Variable
To test the data structure and makeup, a linear regression model is used. This means the target variable must be continuous.

### Outliers
Using Cook's Distance and examining the residual plots, we see significant issues with how a linear model represents the data. With some, the Residual plot has an obvious pattern where it should be more random. The Q-Q plot does not follow the line and has a significant upward curve.
```{r outliers, eval=FALSE, warning=TRUE, include=FALSE}
# Reading model in
load("out/models/lm_mdl.Rdata")

# Calculating Cooks distance on the linear model
cook <- cooks.distance(fit)

# plotting cooks distance to visualise outliers
ggplot(NULL, aes(points, cook)) +
  geom_point() +
  ggrepel::geom_text_repel(aes(), colour = "navy")
```
```{r residual plots, echo=FALSE, warning=FALSE, fig.align='center'}
# Plotting the residual plots for the linear model
load("out/models/lm_mdl.Rdata")
par(mfrow = c(2, 2))
plot(fit)
```
```{r independence, eval=FALSE, warning=FALSE, include=FALSE}
# Checking the independence of the data
car::durbinWatsonTest(fit)
```

As the residual plots showed the data was not well represented by linear modelling, it doesn't make sense to continue testing the linear modelling assumptions against this data. Other tests that could be completed are for homoscedasticity, near zero variance or multicollinearity.

This information has assisted in understanding what regression modelling techniques may be suitable; as such, the models chosen to test will be random forest regression and KNN regression. KNN regression assumes that the closer a data point is to another, the more similar it will be. Random forest regression (decision trees) assumes that data can be split into subsets reasonably distinctly.

# Modelling
The data is split randomly into two groups of testing and training; the testing group is 20% of the observations, and the training is 80%. The training data is then used to build the models; once models are built, the testing data can be used to assess the RMSE and R squared. In the model-building phase, there is also repeated cross-validation. This divides the data into several partitions, with some partitions not used for model building. The unused partitions can then be used as an internal test, and the mean performance is reported across the iterations. The larger data set has been re-sampled to 30,000 observations and split 80/20 into training and testing groups based on compute and time-based factors.
```{r train/ test, warning=FALSE, include=FALSE}

# Splitting the data into training and testing elemnets
split <- resample_partition(dat, c(train = 0.8, test = 0.2))
train <- data.frame(split$train)
test <- data.frame(split$test)
```
```{r rf model, eval=FALSE, warning=FALSE, include=FALSE}
# Setting the range of variables that will be randomly sample at each split
tune <- expand.grid(mtry = seq(1,35,5))

# Running the random forest model on the training data
rfmodel <- train(Price ~ .,
                 data = train,
                 method = "rf",
                 trControl = trainControl("repeatedcv", number = 10, repeats = 3),
                 ntree = 10, 
                 nodesize = 6, #min node size
                 maxnodes = 10, #max node size
                 tuneGrid = tune,
                 importance = TRUE)

# Saving the model output to the out/models folder
save(rfmodel, file = "out/models/rfmodel.Rdata")
```
```{r rf metrics, include=FALSE}
# Reading in the random forest model
load("out/models/rfmodel.Rdata")

# Generating predictions based on the training data
predictions_rf = predict(rfmodel, newdata = test)

# Creating a dataframe that includes the RMSE and R sqaured
rf.metrics <- data.frame(
  model = 'Random Forest',
  RMSE = RMSE(pred = predictions_rf,obs = test$Price),
  R2 = R2(pred = predictions_rf,obs = test$Price)
)

# Creating a variable that has the variable importance plot
rf_varimp <- plot(varImp(rfmodel), top = 10)
```

```{r knn model, eval=FALSE, warning=FALSE, include=FALSE}
# Running the KNN model on the training data
knn_mdl <-  train(Price~ ., 
                  data = train, 
                  method = "knn",
                  preProc = c('center', 'scale'),
                  trControl = trainControl("repeatedcv", number = 10, repeats = 3),
                  tuneLength=5)

# Saving the model into the out/models folder
save(knn_mdl, file = "out/models/.Rdata")
```

```{r knn predictions, include=FALSE}
# Loading the KNN model
load("out/models/knn_mdl.Rdata")
#load("out/models/knn_mdl-withoutfeatures.Rdata")

# Making predsictions of the test data with the KNN model
predictions_knn = predict(knn_mdl, newdata = test)

# Creating a dataframe that contains the RMSE and R sqaured
knn.metrics <- data.frame(
  model = 'KNN',
  RMSE = RMSE(pred = predictions_knn,obs = test$Price),
  R2 = R2(pred = predictions_knn,obs = test$Price)
)
```

# Results
### Random Forest
We can extract a variable importance graph after applying the regression model to the data. This graph shows the variables the model found to be most important in determining price. The variables reported with the highest importance for this data were the type of house being a unit, how many house transfers there had been in the larger area, and the Distance from the CDB and the Southern Metropolitan region. The house being a unit was by far the most important in predicting price, and it could be valuable to investigate the relationship between units, price and the rest of the data to understand better why there is a strong link. 

```{r echo=FALSE, fig.align='center'}
# Plotting the variable importance
plot(varImp(rfmodel), top = 10, main = "Random Forest - Variable Importance", sub= "figure 5")
```

### KNN Regression
The KNN model showed a decrease in RMSE as the number of neighbours was increased. This could show some overfitting with low numbers as it relies too heavily on the data adjacent to each point. The elbow plot indicates that the KNN model performed best with a k value of seven. When a k of 9 was used, the RMSE jumped and decreased again with a k of 13. As the k value increases, the RMSE will also increase, suggesting that k of 7 will be the best value. More hyperparameter tuning could be attempted with a k of 8 or greater than 13 to ensure that the lowest RMSE has been reached. 
```{r plotting knn, echo=FALSE, fig.align='center'}
# Plotting the KNN model
plot(knn_mdl, main = "KNN Elbow Plot", sub = "figure 6")
#plot(varImp(knn_mdl))
```

```{r metrics, include=FALSE}
# Binding the random forest and KNN metrics into one variable
metrics <- rbind(rf.metrics, knn.metrics)

# Formatting and rounding the model metrics
metrics$RMSE <- format(round(as.numeric(metrics$RMSE), 0), nsmall = 0, big.mark = ",")
metrics$R2 <- round(as.numeric(metrics$R2), 2)
train
```

### Comparison
When comparing the two models RMSE and R squared values the random forest returns worse with and RMSE of `r metrics[1,'RMSE']` and an R squared of `r metrics[1,'R2']`. In comparison, the results from the KNN method were `r metrics[2,'RMSE']` and an R squared of `r metrics[2,'R2']`. This shows that neither model is particularly well suited to the data, and our values could better predict house prices.The KNN does have better performance but both could still be improved with further feature engineering and hyper parameter tuning. 

# Conclusion
With neither model showing solid results and the best RMSE of nearly half a million dollars, it can be concluded that this data set is not an accurate predictor of house prices. When assigning the value that the new variables have created, we can investigate the variable importance from the random forest model. Interestingly, the number of house transfers over that period was the second most important when making the predictions. An interesting angle to investigate could be how a quiet market vs a busy market affects house prices. The fifth most important variable was the ratio of vehicles under five years old. A further analysis into the vehicles type (Car, Campervans, Motorcycles) could be interesting in unpacking how and when people spend their money and if there is more of a correlation between vehicles of certain types/ ages associated with areas with higher selling houses. The number of registered vehicles was also seventh in importance, and the data suggests that once the number of registered vehicles exceeds 150,000, house prices will be, on average, less than 1 million dollars. Around 100,000 registered vehicles, with 35% being less than five years old, show a strong cluster with average prices over 1.5 million dollars. The variables for the most popular category and occupation stayed in the data set but didn't have much of a relationship to house price. The total number of jobs had such a small correlation it was dropped earlier in the analysis. 

As a final test, the KNN model was built on a data set that excluded the ABS features. This data set returned an RMSE of 431,310 and an R sqaured of 0.54. This result shows that the ABS data adds additional detail to the modelling process. However, the best results are still inadequate to give a clear picture of the drivers around house pricing.